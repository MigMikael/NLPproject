{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "โครงสร้างคลังข้อความจัดอยู่ในรูปแบบ Isolated เนื่องจากเป็นเพียงแค่ collection ของ text ที่ไม่มีการแบ่งหมวดหมู่ ไม่เกี่ยวข้องกับเวลา ลักษณะคล้ายกับ Guthenberg Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "\n",
    "ชื่อเฉพาะในภาษาอังกฤษมีลักษณะคือขึ้นต้นด้วยตัวพิมพ์ใหญ่ แนวคิดในการหาชื่อเฉพาะจากข้อความของนิยาย Moby Dick คือ\n",
    "1. เลือกเอาเฉพาะคำที่ไม่ซ้ำกันที่ขึ้นต้นด้วยตัว I และเป็นตัวอักษรเท่านั้น (ไม่เอาตัวเลข, สัญลักษณ์พิเศษ)\n",
    "2. นำลิสต์ของคำที่ได้จากข้อหนึ่งมาตัด Stopword ทิ้ง "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_specific_name = set(w for w in text1 if w.startswith('I') and w.istitle() and w.isalpha() and w.lower() not in nltk.corpus.stopwords.words('english') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Iroquois', 'Iceland', 'Immediately', 'Immemorial', 'Italian', 'Instances', 'Instead', 'Israelites', 'Inferable', 'Impossible', 'Inn', 'Innkeeper', 'Islanders', 'Illinois', 'Inward', 'Insufferable', 'Indiaman', 'Impiety', 'Invisible', 'Ixion', 'Icebergs', 'Isthmus', 'Iron', 'Io', 'Israel', 'Issuing', 'Inlanders', 'Indiamen', 'Italy', 'Immense', 'Ifs', 'Interweaving', 'Ishmael', 'Indeed', 'Inserting', 'Isle', 'Iceberg', 'Island', 'Improving', 'Inasmuch', 'Inquisition', 'Immortal', 'India', 'Irish', 'Intolerably', 'Ireland', 'Ingin', 'Imperial', 'Icelandic', 'Inert', 'Imprimis', 'Instantly', 'Ignorance', 'Islands', 'Impenetrable', 'Insurance', 'Indolence', 'Indian', 'Indies', 'Isaiah', 'Icy', 'Isles', 'Isabella', 'Indians', 'Innocents', 'Isolatoes'}\n"
     ]
    }
   ],
   "source": [
    "print(I_specific_name) #ลิสต์ของรายชื่อเฉพาะที่ขึ้นต้นด้วยตัว I "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "print(len(I_specific_name)) # จำนวนของลิสต์รายชื่อเฉพาะที่ขึ้นด้วยตัว I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 \n",
    "คำในลิสต์ข้อ 2.3 ที่คิดว่าเป็นชื่อบุคคลคือ [Ixion](https://en.wikipedia.org/wiki/Ixion), [Isaiah](https://en.wikipedia.org/wiki/Isaiah), Isabella, Ishmael"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    print(\"Number of Token : \", len(text))\n",
    "    print(\"Number of Vocab : \", len(set(text)))\n",
    "    print(\"Lexical Diversity : \", round(len(set(text)) / len(text), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Token :  69342\n",
      "Number of Vocab :  8289\n",
      "Lexical Diversity :  0.12\n"
     ]
    }
   ],
   "source": [
    "adventure_text = brown.words(categories='adventure')\n",
    "adventure_text = [w.lower() for w in adventure_text]\n",
    "lexical_diversity(adventure_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Token :  68488\n",
      "Number of Vocab :  8680\n",
      "Lexical Diversity :  0.13\n"
     ]
    }
   ],
   "source": [
    "fiction_text = brown.words(categories='fiction')\n",
    "fiction_text = [w.lower() for w in fiction_text]\n",
    "lexical_diversity(fiction_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Token :  100554\n",
      "Number of Vocab :  13112\n",
      "Lexical Diversity :  0.13\n"
     ]
    }
   ],
   "source": [
    "news_text = brown.words(categories='news')\n",
    "news_text = [w.lower() for w in news_text]\n",
    "lexical_diversity(news_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Token :  40704\n",
      "Number of Vocab :  8069\n",
      "Lexical Diversity :  0.2\n"
     ]
    }
   ],
   "source": [
    "reviews_text = brown.words(categories='reviews')\n",
    "reviews_text = [w.lower() for w in reviews_text]\n",
    "lexical_diversity(reviews_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Token :  39399\n",
      "Number of Vocab :  5931\n",
      "Lexical Diversity :  0.15\n"
     ]
    }
   ],
   "source": [
    "religion_text = brown.words(categories='religion')\n",
    "religion_text = [w.lower() for w in religion_text]\n",
    "lexical_diversity(religion_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Token :  70022\n",
      "Number of Vocab :  7883\n",
      "Lexical Diversity :  0.11\n"
     ]
    }
   ],
   "source": [
    "romance_text = brown.words(categories='romance')\n",
    "romance_text = [w.lower() for w in romance_text]\n",
    "lexical_diversity(romance_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category ที่มีความหลากหลายของการใช้คำศัพท์น้อยที่สุดคือ __Romance__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unknown(text):\n",
    "    text_vocab = set(w.lower() for w in text if w.isalpha())                       # isalpha() ตัดตัวเลขและสัญลักษณ์พิเศษ\n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.words.words())       # ลิสต์ของคำศัพท์ในภาษาอังกฤษจาก wordlist corpus\n",
    "    stop_word = set(w.lower() for w in stopwords.words('english'))           # ลิสต์ของ stopword จาก stopword corpus\n",
    "    unknown = text_vocab - english_vocab - stop_word                            # คำที่ไม่รู้จัก = คำทั้งหมด - คำที่รู้จัก - stopword\n",
    "    return sorted(unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ตัวอย่างคำ unknown ที่พบสิบตัวแรก :  ['aaa', 'aaaaaaaaah', 'aaaaaaaah', 'aaaaaah', 'aaaah', 'aaaaugh', 'aaagh', 'aaah', 'aaarrrggghhh', 'aaauggh']\n",
      "จำนวนคำ unknown ที่พบ :  7521\n"
     ]
    }
   ],
   "source": [
    "unknown_list = unknown(nltk.corpus.webtext.words())\n",
    "print(\"ตัวอย่างคำ unknown ที่พบสิบตัวแรก : \", unknown_list[:10])\n",
    "print(\"จำนวนคำ unknown ที่พบ : \", len(unknown_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "คำที่มีความยาวมากที่สุดยาว  28  ตัวอักษร\n",
      "\n",
      "สร้างลิสต์สำหรับนับความถี่ของแต่ละความยาวตัวอักษร ตั้งแต่คำที่ยาว 1 ตัวอักษรไปจนถึงยาวที่สุดที่ 28 ตัวอักษร\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \n",
      "\n",
      "ลิสต์ของความถี่ของแต่ละความยาวที่นับได้\n",
      "[0, 9, 131, 370, 523, 877, 1132, 1291, 1025, 836, 526, 334, 169, 119, 47, 42, 28, 15, 5, 7, 10, 4, 7, 3, 5, 1, 3, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for w in unknown_list:\n",
    "    if len(w) > max_len:\n",
    "        max_len = len(w)\n",
    "print(\"คำที่มีความยาวมากที่สุดยาว \", max_len, \" ตัวอักษร\\n\")\n",
    "\n",
    "len_list = [0 for i in range(max_len + 1)]\n",
    "print(\"สร้างลิสต์สำหรับนับความถี่ของแต่ละความยาวตัวอักษร ตั้งแต่คำที่ยาว 1 ตัวอักษรไปจนถึงยาวที่สุดที่ 28 ตัวอักษร\")\n",
    "print(len_list, '\\n')\n",
    "\n",
    "for w in unknown_list:\n",
    "    len_list[len(w)]+=1\n",
    "    \n",
    "print(\"ลิสต์ของความถี่ของแต่ละความยาวที่นับได้\")\n",
    "print(len_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 คำที่มีความยาว 7 ตัวอักษรมีความถี่สูงสุดอยู่ที่ 1291\n",
    "### 5.3 คำที่มีความยาว 25, 27, 28 ตัวอักษรมีความถี่ต่ำสุดอยู่ที่ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'fox', 'the', 'dog']"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"The quick brown fox jumps over the lazy dog\"\n",
    "[w.lower() for w in sent.split() if len(w) == 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('united', 'state'), ('state', 'of'), ('of', 'america')]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['united', 'state', 'of', 'america']     # ทดสอบการใช้ bigram\n",
    "list(nltk.bigrams(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "import operator\n",
    "\n",
    "def my_bigram(text, word):\n",
    "    bigram_list = [tu for tu in nltk.bigrams(text) if tu[1] == word]     # คัดเอาเฉพาะที่ลงท้ายด้วย word \n",
    "    bigram_dict = {tu : 0 for tu in bigram_list}                                 # สร้าง Dictionary สำหรับเก็บความถี่ของแต่ละ bigram\n",
    "    \n",
    "    for tu in bigram_list:                                                                 # นับความถี่ของแต่ละ bigram\n",
    "        bigram_dict[tu]+=1\n",
    "    \n",
    "    sorted_bigram_dict = sorted(bigram_dict.items(), key=operator.itemgetter(1), reverse=True)     # เรียงลำดับตามความถี่ที่นับได้จากมากไปน้อย\n",
    "    return sorted_bigram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('United', 'States'), 153),\n",
       " (('the', 'States'), 77),\n",
       " (('several', 'States'), 8),\n",
       " (('of', 'States'), 6),\n",
       " (('Southern', 'States'), 6),\n",
       " (('new', 'States'), 5),\n",
       " (('thirteen', 'States'), 5),\n",
       " (('those', 'States'), 5),\n",
       " (('these', 'States'), 4),\n",
       " (('original', 'States'), 3)]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_bigram(inaugural.words(), 'States')[:10]                                     # ทดสอบใช้งานกับคำว่า States                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 คำว่า citizens ห้าอันดับแรก"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('every', 'citizen'), 16),\n",
       " (('the', 'citizen'), 11),\n",
       " (('a', 'citizen'), 6),\n",
       " (('American', 'citizen'), 6),\n",
       " (('other', 'citizen'), 2)]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_bigram(inaugural.words(), 'citizen')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 คำว่า people ห้าอันดับแรก"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('the', 'people'), 243),\n",
       " (('our', 'people'), 73),\n",
       " (('American', 'people'), 39),\n",
       " (('a', 'people'), 23),\n",
       " (('The', 'people'), 16)]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_bigram(inaugural.words(), 'people')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 คำว่า government ห้าอันดับแรก"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('of', 'government'), 65),\n",
       " (('-', 'government'), 30),\n",
       " (('a', 'government'), 24),\n",
       " (('the', 'government'), 15),\n",
       " (('free', 'government'), 11)]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_bigram(inaugural.words(), 'government')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('cetacean.n.01'), Synset('cetacean.a.01')]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('cetacean')                                                     # ตรวจสอบว่ามีได้กี่รูปแบบ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'large aquatic carnivorous mammal with fin-like forelimbs no hind limbs, including: whales; dolphins; porpoises; narwhals'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('cetacean.n.01').definition()                              # ความหมาย"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cetacean', 'cetacean_mammal', 'blower']"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('cetacean.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### คำที่อยู่ใน synset เดียวกันมีทั้งหมด 3 คำ ได้แก่ cetacean, cetacean mammal, blower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 (ข้อนี้ยอมแพ้ครับ T_T) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('animal.n.01'), Synset('animal.s.01')]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('animal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('animal_tissue.n.01')]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('animal.n.01').substance_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('homosexual.n.01'), Synset('homo.n.02')]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('homo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dolphinfish.n.02'), Synset('dolphin.n.02')]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dolphin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string                   # ทดสอบ\n",
    "string.ascii_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appear :  2\n",
      "hierarchy :  3\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import cmudict\n",
    "\n",
    "cmu_dict = dict(cmudict.entries())\n",
    "\n",
    "def syllable(word):\n",
    "    if word in cmu_dict:\n",
    "        return len([sound for sound in cmu_dict[word] if sound.strip(string.ascii_letters)])\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "print(\"appear : \", syllable(\"appear\"))       # ทดสอบด้วยคำว่า appear\n",
    "print(\"hierarchy : \", syllable(\"hierarchy\"))    # ทดสอบอีกทีด้วย hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1 จำนวนพยางค์ที่ยาวที่สุด"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "จำนวนพยางค์ที่ยาวที่สุดใน inaugural :  7\n"
     ]
    }
   ],
   "source": [
    "max_syllable = 0\n",
    "\n",
    "for w in inaugural.words():\n",
    "    current_syllable = syllable(w)\n",
    "    if current_syllable > max_syllable:\n",
    "        max_syllable = current_syllable\n",
    "        \n",
    "print('จำนวนพยางค์ที่ยาวที่สุดใน inaugural : ', max_syllable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[23419, 79783, 24683, 11516, 4921, 1230, 179, 4]\n"
     ]
    }
   ],
   "source": [
    "syllable_freq = [0 for i in range(max_syllable + 1)]\n",
    "print(syllable_freq)\n",
    "\n",
    "for w in inaugural.words():\n",
    "    current_syllable = syllable(w)\n",
    "    syllable_freq[current_syllable] += 1\n",
    "    \n",
    "print(syllable_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### จำนวนพยางค์ที่มีความถี่สูงสุดอยู่ที่ 1 พยางค์ มีความถี่ 79783"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79783, 24683, 11516, 4921, 1230, 179, 4]\n",
      "จำนวนพยางค์ทั้งหมด :  122316\n",
      "จำนวนคำทั้งหมด :  145735\n",
      "ค่าเฉลี่ยจำนวนพยางค์ต่อคำ :  0.8393042165574501\n"
     ]
    }
   ],
   "source": [
    "# หาจำนวนพยางค์จากลิสต์ syllable_freq ของข้อที่แล้ว อย่างไรก็ตามต้องตัดคำที่มี 0 พยางค์ทิ้งเนื่องจาก 0 พยางค์ไม่นับเป็นพยางค์ (ตัดลิสต์ช่องแรก)\n",
    "slice_syllable_freq = syllable_freq[1:8]\n",
    "\n",
    "print(slice_syllable_freq)\n",
    "print(\"จำนวนพยางค์ทั้งหมด : \", sum(slice_syllable_freq))\n",
    "print(\"จำนวนคำทั้งหมด : \", len(inaugural.words()))\n",
    "print(\"ค่าเฉลี่ยจำนวนพยางค์ต่อคำ : \", sum(slice_syllable_freq)/len(inaugural.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1\n",
    "\n",
    "สูตรคือ RE = 206.835 – (1.015 x ASL) – (84.6 x ASW)\n",
    "\n",
    "เมื่อ \n",
    "- RE คือ Readability Ease \n",
    "- ASL คือ Average Sentence Length หรือความยาวโดยเฉลี่ยของประโยค __หาได้จากการนำจำนวนคำทั้งหมดหารด้วยจำนวนประโยคทั้งหมด__\n",
    "- ASW คือ Average of syllable per word หรือจำนวนพยางค์ในแต่ละคำ __หาได้จากการนำจำนวนพยางค์ทั้งหมดหารด้วยจำนวนคำทั้งหมด__\n",
    "\n",
    "การตีความค่า RE ที่คำนวนได้ใช้เกณฑ์ดังนี้\n",
    "- ถ้าค่า RE อยู่ระหว่าง 90 - 100 จะเป็นข้อความที่เด็กเกรด 5 เข้าใจได้ (ประมาณเด็ก ป.5)\n",
    "- ถ้าค่า RE อยู่ระหว่าง 60 - 70 จะเป็นข้อความที่เด็กเกรด 8 หรือ 9 เข้าใจได้ (ประมาณเด็ก ม.2 ม.3)\n",
    "- ถ้าค่า RE อยู่ระหว่าง 0 - 30 จะเป็นข้อความที่เด็กมหาลัยเข้าใจได้\n",
    "\n",
    "ที่มา [The Flesch Reading Ease Readability Formula](http://www.readabilityformulas.com/flesch-reading-ease-readability-formula.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.05995433789957"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility \" \\\n",
    "       \"of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. \" \\\n",
    "       \"Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating\"\\\n",
    "       \" sets of solutions for all types of systems are given. These criteria and the corresponding algorithms \" \\\n",
    "       \"for constructing a minimal supporting set of solutions can be used in solving all the considered types of \" \\\n",
    "       \"systems and systems of mixed types.\"\n",
    "    \n",
    "text2 = \"Filmed over 12 years with the same cast, Richard Linklater's BOYHOOD is a groundbreaking story of growing up as seen through the eyes of a child named Mason (a breakthrough performance by Ellar Coltrane), who literally grows up on screen before our eyes. Starring Ethan Hawke and Patricia Arquette as Mason's parents and newcomer Lorelei Linklater as his sister Samantha, BOYHOOD charts the rocky terrain of childhood like no other film has before. Snapshots of adolescence from road trips and family dinners to birthdays and graduations and all the moments in between become transcendent, set to a soundtrack spanning the years from Coldplay's Yellow to Arcade Fire's Deep Blue. BOYHOOD is both a nostalgic time capsule of the recent past and an ode to growing up and parenting. It's impossible to watch Mason and his family without thinking about our own journey. (c) Sundance Film Fest\"\n",
    "    \n",
    "# ฟังก์ชั่นสำหรับแบ่งประโยค\n",
    "def split_sentence(text):\n",
    "    sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', text)\n",
    "    return sentences\n",
    "\n",
    "# ฟังก์ชั่นสำหรับแบ่งคำ\n",
    "def split_word(text):\n",
    "    return text.split()\n",
    "\n",
    "# คำนวน RE\n",
    "def readability_ease(text):\n",
    "    sentences = split_sentence(text)\n",
    "    sum_words = 0                                         \n",
    "    for sentence in sentences:                                     # หาจำนวนคำในแต่ละประโยค\n",
    "        sum_words += len(sentence.split())\n",
    "        \n",
    "    ASL = sum_words / len(sentences)                         # จำนวนคำทั้งหมดหารด้วยจำนวนประโยค\n",
    "    #print(ASL)\n",
    "    \n",
    "    sum_words_syllable = 0\n",
    "    words = split_word(text)\n",
    "    for word in words:                                               # หาจำนวนพยางค์ทั้งหมด\n",
    "        sum_words_syllable += syllable(word)\n",
    "    \n",
    "    ASW = sum_words_syllable / len(words)                # จำนวนพยางค์ทั้งหมดหารด้วยจำนวนคำ\n",
    "    #print(ASW)\n",
    "    \n",
    "    RE = 206.835 - (1.015 * ASL) - (84.6 * ASW)         # คำนวน RE\n",
    "    #print(RE)\n",
    "    \n",
    "    return RE\n",
    "    \n",
    "readability_ease(text2)                                              # ลองใช้กับ text2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.05995433789957\n",
      "Grade 5 or below\n"
     ]
    }
   ],
   "source": [
    "def suitable_for(text):\n",
    "    RE = readability_ease(text)\n",
    "    if RE >= 90:\n",
    "        return \"Grade 5 or below\"\n",
    "    elif RE >= 70:\n",
    "        return \"Grade 6 or 7\"\n",
    "    elif RE >= 60:\n",
    "        return \"Grade 8 or 9\"\n",
    "    elif RE >= 0:\n",
    "        return \"College graduates\"\n",
    "    \n",
    "print(readability_ease(text2))\n",
    "print(suitable_for(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "skottler\n",
      "March 01, 2018\n",
      "On Wednesday, February 28, 2018 GitHub.com was unavailable from 17:21 to 17:26 UTC and intermittently unavailable from 17:26 to 17:30 UTC due to a distributed denial-of-service (DDoS) attack. We understand how much you rely on GitHub and we know the availability of our service is of critical importance to our users. To note, at no point was the confidentiality or integrity of your data at risk. We are sorry for the impact of this incident and would like to describe the event, the efforts we’ve taken to drive availability, and how we aim to improve response and mitigation moving forward.Cloudflare described an amplification vector using memcached over UDP in their blog post this week, “Memcrashed - Major amplification attacks from UDP port 11211”. The attack works by abusing memcached instances that are inadvertently accessible on the public internet with UDP support enabled. Spoofing of IP addresses allows memcached’s responses to be targeted against another address, like ones used to serve GitHub.com, and send more data toward the target than needs to be sent by the unspoofed source. The vulnerability via misconfiguration described in the post is somewhat unique amongst that class of attacks because the amplification factor is up to 51,000, meaning that for each byte sent by the attacker, up to 51KB is sent toward the target.Over the past year we have deployed additional transit to our facilities. We’ve more than doubled our transit capacity during that time, which has allowed us to withstand certain volumetric attacks without impact to users. We’re continuing to deploy additional transit capacity and develop robust peering relationships across a diverse set of exchanges. Even still, attacks like this sometimes require the help of partners with larger transit networks to provide blocking and filtering.Between 17:21 and 17:30 UTC on February 28th we identified and mitigated a significant volumetric DDoS attack. The attack originated from over a thousand different autonomous systems (ASNs) across tens of thousands of unique endpoints. It was an amplification attack using the memcached-based approach described above that peaked at 1.35Tbps via 126.9 million packets per second.At 17:21 UTC our network monitoring system detected an anomaly in the ratio of ingress to egress traffic and notified the on-call engineer and others in our chat system. This graph shows inbound versus outbound throughput over transit links:Given the increase in inbound transit bandwidth to over 100Gbps in one of our facilities, the decision was made to move traffic to Akamai, who could help provide additional edge network capacity. At 17:26 UTC the command was initiated via our ChatOps tooling to withdraw BGP announcements over transit providers and announce AS36459 exclusively over our links to Akamai. Routes reconverged in the next few minutes and access control lists mitigated the attack at their border. Monitoring of transit bandwidth levels and load balancer response codes indicated a full recovery at 17:30 UTC. At 17:34 UTC routes to internet exchanges were withdrawn as a follow-up to shift an additional 40Gbps away from our edge.The first portion of the attack peaked at 1.35Tbps and there was a second 400Gbps spike a little after 18:00 UTC. This graph provided by Akamai shows inbound traffic in bits per second that reached their edge:Making GitHub’s edge infrastructure more resilient to current and future conditions of the internet and less dependent upon human involvement requires better automated intervention. We’re investigating the use of our monitoring infrastructure to automate enabling DDoS mitigation providers and will continue to measure our response times to incidents like this with a goal of reducing mean time to recovery (MTTR).We’re going to continue to expand our edge network and strive to identify and mitigate new attack vectors before they affect your workflow on GitHub.com.We know how much you rely on GitHub for your projects and businesses to succeed. We will continue to analyze this and other events that impact our availability, build better detection systems, and streamline response.\n",
      "    Manager, Site Reliability Engineering\n",
      "  \n",
      "GitHub ProfileTwitter Profile\n",
      "For updates, follow us on Twitter or join our team. Check out the feed if you do the RSS thing. with  by \n"
     ]
    }
   ],
   "source": [
    "# ดึงเฉพาะส่วนข้อความออกมา\n",
    "\n",
    "from urllib import request\n",
    "\n",
    "url2 = \"https://githubengineering.com/ddos-incident-report/\"\n",
    "html = request.urlopen(url2).read().decode('utf-8')\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "all_p_tag = soup.find_all('p')\n",
    "\n",
    "the_text = \"\"\n",
    "for p in all_p_tag:\n",
    "    the_text += p.get_text()\n",
    "    \n",
    "print(the_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ค่า Readability Ease :  75.10144468882373\n",
      "เหมาะสำหรับเด็ก :  Grade 6 or 7\n"
     ]
    }
   ],
   "source": [
    "print(\"ค่า Readability Ease : \", readability_ease(the_text))\n",
    "print(\"เหมาะสำหรับเด็ก : \", suitable_for(the_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
